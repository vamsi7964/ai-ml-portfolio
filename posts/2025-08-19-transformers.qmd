---
title: "Transformers, explained simply (with just enough math)"
date: 2025-08-19
categories: [NLP, Transformers, Fundamentals]
image: assets/og-image.png
---

> A transformer is a pattern-matching machine powered by attention. Here's the shortest path from idea → intuition → math.

## Intuition first
- **Tokens** are chunks of text turned into vectors.
- **Attention** lets each token look at the others and decide which ones matter.
- **Stack layers** of this to build rich context.

## The core equations (briefly)
Given queries $Q$, keys $K$, and values $V$:

$$
\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
$$

- The softmax scores say *how much* each other token matters.
- We use multiple heads to look for different patterns in parallel.

## Try it in Python

```{python}
import numpy as np

def scaled_dot_attn(Q,K,V):
    scores = Q @ K.T / np.sqrt(Q.shape[-1])
    W = np.exp(scores - scores.max(axis=-1, keepdims=True))
    W = W / W.sum(axis=-1, keepdims=True)
    return W @ V

d = 4
Q = np.random.randn(3,d); K = np.random.randn(3,d); V = np.random.randn(3,d)
out = scaled_dot_attn(Q,K,V)
out.shape
```
