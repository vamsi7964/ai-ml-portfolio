---
title: "RoPE vs. absolute positional embeddings (quick visual)"
date: 2025-08-19
categories: [NLP, Embeddings, Positional-Encoding]
image: assets/og-image.png
---

**TL;DR**: Absolute positional embeddings add a learned offset per position. **RoPE** rotates pairs of feature dimensions by an angle that depends on the position, letting relative positions emerge *inside* attention.

### One-liner intuition
- Absolute: "This token is at index 12."
- RoPE: "The *relative* angle between tokens carries the position info."

```{python}
import numpy as np

def rope_rotate(x, theta):
    x = x.reshape(-1,2)
    c, s = np.cos(theta), np.sin(theta)
    R = np.array([[c,-s],[s,c]])
    return (x @ R).reshape(-1)

x = np.array([1.0,0.0, 0.5,0.5])
y = rope_rotate(x, theta=0.3)
z = rope_rotate(x, theta=0.6)
(y, z)
```
