{
  "hash": "53da47e6d286ee0f940b4a040ea4020a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Transformers, explained simply (with just enough math)\"\ndate: 2025-08-19\ncategories: [NLP, Transformers, Fundamentals]\nimage: assets/og-image.png\n---\n\n> A transformer is a pattern-matching machine powered by attention. Here's the shortest path from idea → intuition → math.\n\n## Intuition first\n- **Tokens** are chunks of text turned into vectors.\n- **Attention** lets each token look at the others and decide which ones matter.\n- **Stack layers** of this to build rich context.\n\n## The core equations (briefly)\nGiven queries $Q$, keys $K$, and values $V$:\n\n$$\n\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n$$\n\n- The softmax scores say *how much* each other token matters.\n- We use multiple heads to look for different patterns in parallel.\n\n## Try it in Python\n\n::: {#390fe02e .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\ndef scaled_dot_attn(Q,K,V):\n    scores = Q @ K.T / np.sqrt(Q.shape[-1])\n    W = np.exp(scores - scores.max(axis=-1, keepdims=True))\n    W = W / W.sum(axis=-1, keepdims=True)\n    return W @ V\n\nd = 4\nQ = np.random.randn(3,d); K = np.random.randn(3,d); V = np.random.randn(3,d)\nout = scaled_dot_attn(Q,K,V)\nout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(3, 4)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "2025-08-19-transformers_files"
    ],
    "filters": [],
    "includes": {}
  }
}